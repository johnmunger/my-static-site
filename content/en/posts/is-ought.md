---
title: "The Is-Ought Problem"
date: 2022-07-05T06:19:39-05:00
description:
draft: true
hideToc: true
enableToc: true
enableTocContent: false
tocFolding: false
tocPosition: inner
tocLevels: ["h2", "h3", "h4"]
tags:
-
series:
-
categories:
-
image:
---
#### Or the Naturalist Fallacy

Maybe its because I'm a coder, maybe its because I was once an atheist naively trying to find an objective morality to justify my moral grounding to religious parents.  Motivations aside, when I grasped the significance of the simple logic behind the is-ought problem, it changed the way I saw the world permanently.

#### What is the is-ought problem?

Put in simplest terms, the is-ought problem is the assertion that one cannot derive moral conclusions from descriptive facts.  You can't get values from facts. You can't get ought from is.  Another way of putting it is, something being natural does not imply it is right.  So far, so boring right?  Well, astute readers may have already noticed a micro-volcano erupting somewhere deep inside of them - usually in the gut - sometimes the throat. This realization caused me to ask: How do we distinguish right from wrong? Is it all opinion?  Can there be no objective truth to the morality? But even if all morality ultimately boils down to opinion, WHY THEN, why do we have such STRONG moral intuitions?  How can we be so convinced of assertions that are ultimately opinion?  What is substance of moral reality?

#### First Principles

First, let's make an important observation.  Moral claims don't have to be sweeping assertions about what it means to be a good person.  This is perhaps the greatest difficulty I encounter when trying to explain the significance of this problem.  The word moral comes with a very specific connotation in common parlance, but A moral claim is any assertion of preference.  ANY sentence with the word should or ought is, technically, a moral claim.  "You SHOULD grind the coffee beans before placing them in your coffee filter." is a moral claim.  Its not exactly a religious axiom, or worthy of being tattoo'd on your body, but it doesn't have to be.  The important fact about moral claims is that they tell you how to ACT in the world.  The aren't descriptions of the world.  They are instructions.

With that in mind, let's view the problem of morality from a different vantage point.  Let's assume we are making a robot.  Your robot comes with no instructions on how to act.  You have to have to program this robot.  Before this robot can do anything it needs to know how to move, and how to see, and how distinguish one thing from the other.  In fact, it won't know what a thing is unless you define it.

Let's say you want to make a robot that picks blueberries.  Let's say your robot looks like WALL-E.  He has two cameras for eyes, and some rotating tracks like a tank to move.  You put him in the blueberry patch, and hit "go".  WALL-E receives a stream of pixel data that feeds into his CPU.  But WALL-E doesn't know what pixels correspond to a blueberry.  "Ah! we'll write a program to identify a blueberry based on a bunch of blueberries"  We write the program.  WALL-E does nothing. We look at WALL-E's logs.  We see that he has identified a bunch of blueberries, but he doesn't know how to get to the blueberries. "Ah! we'll write a program that makes him move toward a blueberry, stop within arm's reach, extend WALL-E's arm, and then grab the blueberry." We write the program, and hit go. WALL-E moves towards blueberry's, but runs into a rock and falls over.  This is going to be a lot harder than we thought.  Alright now we need to gives some instructions for how to identify impediments WALL-E might encounter, and how to avoid them.  As you can see we are beginning to shape a morality for WALL-E, albeit, a very basic one.  We could call this WALL-E's bodily morality. This is how WALL-E SHOULD operate his body, if he wants to pick blueberries. This is how WALL-E SHOULD act if he wants to accomplish his physical task without harming himself.  

Let's assume we create a few more WALL-Es and have them all trying to pick blueberries at the same time.  Now, the obstacles WALL-E faces are the behaviors, of other WALL-Es. the content of this morality will consist of instances of conflict and what TO DO when we encounter them.  "What happens if another WALL-E is grabbing the blueberry?" Ah, simple just give your fellow WALL-E's a few feet of space at all times.

Let's assume we manage to give the WALL-Es a survival instinct. The program might say, "if executing your next instruction would kill you, try fight or try run, and ignore all other rules.  Then lets introduce, another rule for blueberry collection.  You must collect 10,000 blueberries per day, or you die. What do our WALL-Es do then?

#### Moral Substrate

If there is a substrate to morality it would be this social decision-space.  The space of all possible decisions about what one can do.  As in the WALL-E example, it is probably worth distinguishing between rules that tell one how to act to accomplish the actual nuts and bolts of life - i.e. moving around, picking stuff up, putting it down, drinking, eating, physical skills - and the rules on how to act with other people.  This is the space that philosophers have largely confined themselves to.  Morality, conventionally, is how to act in various social contexts.  